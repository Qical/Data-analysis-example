import jieba
import jieba.posseg as pseg # ç»Ÿè®¡è¯æ€§
import pandas as pd
import re
from collections import Counter # ç”¨äºå­—å…¸ç±»å‹çš„è®¡æ•°


# ç”Ÿæˆechartsç±»åº“è¡¨çš„è°ƒç”¨
from pyecharts.globals import SymbolType
from pyecharts.charts import Bar,WordCloud

# è¯»å–ä¸‹è½½å¥½çš„æ–‡ä»¶
df = pd.read_csv("Jay_comment_data.csv")
# å°†è¯„è®ºè¿™ä¸€åˆ—æå–å‡ºæ¥å¹¶ä¸”è½¬æ¢æˆåˆ—è¡¨,\
# è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯df[].valuesæ˜¯ndarrayçš„æ ¼å¼ï¼Œè¿˜éœ€è¦æ·»åŠ tolist()çš„æ–¹æ³•
comment_list = df['comment'].values.tolist()

# åˆ›å»ºå­—å…¸ï¼Œæ‹†è¯è®¡æ•°
word_dict = {}

for line in comment_list:
    words = jieba.cut(line) #è¿™é‡Œçš„wordæ˜¯è¿­ä»£å™¨
    for word in words:
        if word not in word_dict:
            word_dict[word] = 1
        else:
            word_dict[word] += 1

# å¤„ç†åˆ†è¯
stop_word = pd.read_csv("Chinese_Stopwords.txt",encoding='utf-8',header = None)
# å› ä¸ºæ˜¯pandasè¯»å–ï¼Œå¸®å…¶æ›´æ–°ç›¸åº”çš„åˆ—å
stop_word.columns = ['word']
stop_word = [' '] +list(stop_word['word'])

# å°†å¸¸ç”¨çš„â€œåºŸè¯â€å»æ‰
for i in range(len(stop_word)):
    if stop_word[i] in word_dict:
        word_dict.pop(stop_word[i])

# æŒ‰ç…§è¯é¢‘å‡ºç°çš„æ¬¡æ•°è¿›è¡Œæ’åº,sortedè¦ä¼ å…¥ä¸€ä¸ªå¯è¿­ä»£çš„å¯¹è±¡ï¼Œæ‰€ä»¥ä½¿ç”¨word_dict.items()æ„å»ºåˆ—è¡¨
word_dict_sort = sorted(word_dict.items(),key = lambda x:x[1],reverse=True )

# å–å‰100çš„è¯é¢‘
word = word_dict_sort[:100]

# åˆ¶ä½œè¯äº‘å›¾
wordcloud = WordCloud()
# æ·»åŠ æ³¨é‡Š
wordcloud.add("Jay",words,word_size_range=[20,100],shape='circle')
wordcloud.render_notebook()

# è¿›è¡Œè¯æ€§åˆ†ç±»
def speech_cut(speech):
    word_list =[]
    for word in word_dict_sort:
        words = pseg.cut(word[0])
        for w,flag in words:
            if flag == speech:
                word_list.append(word)
    return word_list

# åŠ¨è¯ V
verb_word = speech_cut('v')
wordcloud = WordCloud()
wordcloud.add("",verb_word[:100],word_size_range=[20,100],shape='circle')
wordcloud.render_notebook()

# åè¯ n
verb_word = speech_cut('n')
wordcloud = WordCloud()
wordcloud.add("",verb_word[:100],word_size_range=[20,100],shape='circle')
wordcloud.render_notebook()


# è¡¨æƒ…åˆ†æ,çˆ¬ä¸‹æ¥çš„æ–‡æœ¬ä¼šæœ‰emojiï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è¿‡æ»¤æ‰
"""
å…·ä½“çš„mysqlçš„å¤„ç†æ–¹å¼å¯ä»¥é€šè¿‡å¦‚ä¸‹çš„é“¾æ¥æŸ¥çœ‹
https://blog.csdn.net/weixin_42812527/article/details/81876713
æœ‰æ—¶éœ€è¦è¿‡æ»¤æ‰å››å­—èŠ‚ä»¥ä¸Šçš„å­—ç¬¦ï¼ˆè¡¨æƒ…ï¼‰ï¼Œæ¯”å¦‚MySQLæ•°æ®åº“5.5.3ä»¥ä¸‹çš„ç‰ˆæœ¬textå­—æ®µä¸æ”¯æŒå››å­—èŠ‚ä»¥ä¸Šå­—ç¬¦
"""

def get_emoji(content):
    pattern = re.compile(u"[\U00010000-\U0010ffff]")
    result = re.findall(pattern,content)
    return result

# applyçš„ä¼ å‚æ–¹æ³•éœ€è¦æ³¨æ„ä¸€ä¸‹ï¼š
df['emojis_list'] = df['comment'].apply(get_emoji)
# è½¬å˜æˆåˆ—è¡¨(æ²¡æœ‰emojiå°±æ˜¯ç©ºåˆ—è¡¨ï¼Œæœ‰emojiå°±æ˜¯emojiè¡¨æƒ…
# eg:[[], [], ['ğŸ˜­'], ['ğŸ˜˜'], [], [], [], ['ğŸ˜'], ['ğŸ˜­'], ['ğŸŒ¹', 'ğŸŒ¹', 'ğŸŒ¹', 'ğŸŒ¹']]
emojis = df['emojis_list'].values.tolist()

# è¿™ä¸ªåœ°æ–¹å¦™ç”¨äº†sumçš„æ–¹æ³•ï¼Œç»™ä¾‹è¡¨å®ç°äº†é™ç»´ï¼Œå¹¶ä¸”å»æ‰äº†ç©ºåˆ—è¡¨[],æœ¬åº”è¯¥ç”¨joinçš„æ–¹å¼æ¥å®ç°
# https://blog.csdn.net/BF02jgtRS00XKtCx/article/details/89484064å¯ä»¥å¤§è‡´å‚è€ƒè¿™ä¸ªåšå®¢è¿›è¡ŒæŸ¥çœ‹
emojis_list = sum(emojis,[])

# å¯¹åˆ—è¡¨çš„å…ƒç´ è¿›è¡Œè®¡æ•°ï¼Œå¹¶ä¸”ç”Ÿæˆå­—å…¸
# eg: {'ğŸ˜­': 261, 'ğŸ˜': 83, 'ğŸ˜ƒ': 79, 'ğŸ˜‚': 69, 'ğŸ‘': 67}
counter = Counter(emojis_list)
# most_common([n]):http://www.pythoner.com/205.html
# è¿”å›ä¸€ä¸ªTopNåˆ—è¡¨ã€‚å¦‚æœnæ²¡æœ‰è¢«æŒ‡å®šï¼Œåˆ™è¿”å›æ‰€æœ‰å…ƒç´ ã€‚å½“å¤šä¸ªå…ƒç´ è®¡æ•°å€¼ç›¸åŒæ—¶ï¼Œæ’åˆ—æ˜¯æ— ç¡®å®šé¡ºåºçš„ã€‚
y_emojis, x_counts = zip(*counter.most_common())
bar = Bar()
bar.add_xaxis(y_emojis[:20])
bar.add_yaxis("", x_counts[:20])
bar.render_notebook()
